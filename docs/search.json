[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "이재혁 (Jae Hyeok Lee)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jhyeok-lee.github.io",
    "section": "",
    "text": "Language Modeling with nn.Transformer and Torchtext\n\n\n\n\n\n\n\npytorch\n\n\npytorch-tutorial\n\n\nnlp\n\n\n\n\n\n\n\n\n\n\n\nMar 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPytorch Datasets\n\n\n\n\n\n\n\npytorch\n\n\npytorch-tutorial\n\n\npytorch-dataset\n\n\n\n\n\n\n\n\n\n\n\nMar 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPytorch Quickstart\n\n\n\n\n\n\n\npytorch\n\n\npytorch-tutorial\n\n\n\n\n\n\n\n\n\n\n\nMar 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPytorch Tensors\n\n\n\n\n\n\n\npytorch\n\n\npytorch-tutorial\n\n\npytorch-tensor\n\n\n\n\n\n\n\n\n\n\n\nMar 5, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/pytorch-lm-with-nntransformer-and-torchtext/pytorch-lm-with-nntransformer-and-torchtext.html",
    "href": "posts/pytorch-lm-with-nntransformer-and-torchtext/pytorch-lm-with-nntransformer-and-torchtext.html",
    "title": "Language Modeling with nn.Transformer and Torchtext",
    "section": "",
    "text": "출처 : PyTorch Transformer\n이 튜토리얼은 nn.Transformer 모듈을 사용해서 seq2seq 학습에 대해 다룬다.\nPyTorch 1.2부터 Attention is All You Need 논문에 기반한 표준 transformer 모듈이 포함된다. Recurrent Neural Networks (RNNs)와 비교해서, transformer 모델이 seq2seq 작업에 더 효과적이라는 것이 증명되었다. nn.Transformer 모듈은 input과 output 사이의 전역 의존성을 이끌어내기 위해 attention mechchanism (nn.MultiheadAttention에서 구현)을 사용한다. nn.Transformer 모듈은 고도로 모듈화가 되어 있어 단일 컴포넌트 (예, nn.TransformerEncoder)를 쉽게 조정하거나 구성할 수 있다."
  },
  {
    "objectID": "posts/pytorch-lm-with-nntransformer-and-torchtext/pytorch-lm-with-nntransformer-and-torchtext.html#define-the-model",
    "href": "posts/pytorch-lm-with-nntransformer-and-torchtext/pytorch-lm-with-nntransformer-and-torchtext.html#define-the-model",
    "title": "Language Modeling with nn.Transformer and Torchtext",
    "section": "Define the model",
    "text": "Define the model\n이 튜토리얼에서, language modeling 작업용으로 nn.TransformerEncoder를 학습한다. Language modeling 작업이란 주어진 단어(또는 단어 sequence)가 앞의 단어 sequence를 따를 가능성에 대한 확률을 구한다. 우선 토큰 sequence는 embedding layer로 전달된 다음, 단어 순서를 설명하기 위해 positional encoding layer로 전달된다 (자세한 설명은 다음 문단에). nn.TransformerEncoder는 여러 개의 nn.TransformerEncoderLayer로 이루어져 있다. input sequence와 함께 정사각형 attention mask가 필요한데, 왜냐하면 nn.TransformerEncoder의 self-attention layer는 input sequence의 앞부분에만 적용되기 때문이다. Language modeling 작업에서 뒷부분의 토큰은 마스킹되어야 한다. output 단어들의 확률분포를 생성하기 위해서는, nn.TransformerEncoder의 output은 linear layer를 통과한 후 log-softmax function을 거친다.\n\nimport math\nimport os\nfrom tempfile import TemporaryDirectory\nfrom typing import Tuple\n\nimport torch\nfrom torch import nn, Tensor\nimport torch.nn.functional as F\nfrom torch.nn import TransformerEncoder, TransformerEncoderLayer\nfrom torch.utils.data import dataset\n\nclass TransformerModel(nn.Module):\n\n    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n                 nlayers: int, dropout: float = 0.5):\n        super().__init__()\n        self.model_type = 'Transformer'\n        self.pos_encoder = PositionalEncoding(d_model, dropout)\n        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n        self.encoder = nn.Embedding(ntoken, d_model)\n        self.d_model = d_model\n        self.decoder = nn.Linear(d_model, ntoken)\n\n        self.init_weights()\n\n    def init_weights(self) -> None:\n        initrange = 0.1\n        self.encoder.weight.data.uniform_(-initrange, initrange)\n        self.decoder.bias.data.zero_()\n        self.decoder.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n        \"\"\"\n        Args:\n            src: Tensor, shape [seq_len, batch_size]\n            src_mask: Tensor, shape [seq_len, seq_len]\n\n        Returns:\n            output Tensor of shape [seq_len, batch_size, ntoken]\n        \"\"\"\n        src = self.encoder(src) * math.sqrt(self.d_model)\n        src = self.pos_encoder(src)\n        output = self.transformer_encoder(src, src_mask)\n        output = self.decoder(output)\n        return output\n\n\ndef generate_square_subsequent_mask(sz: int) -> Tensor:\n    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n\nPositionalEncoding 모듈은 sequence에 있는 토큰들의 상대적이거나 절대적인 위치에 대한 정보를 주입한다. Positional encodings는 embeddings와 동일한 dimension을 갖고 있어서 같이 더할 수 있다. 아래의 코드는 sin, cos functions를 사용해서 positional encoding에 대한 구현이다.\n\nclass PositionalEncoding(nn.Module):\n\n    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        position = torch.arange(max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe = torch.zeros(max_len, 1, d_model)\n        pe[:, 0, 0::2] = torch.sin(position * div_term)\n        pe[:, 0, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x: Tensor) -> Tensor:\n        \"\"\"\n        Args:\n            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n        \"\"\"\n        x = x + self.pe[:x.size(0)]\n        return self.dropout(x)"
  },
  {
    "objectID": "posts/pytorch-lm-with-nntransformer-and-torchtext/pytorch-lm-with-nntransformer-and-torchtext.html#load-and-batch-data",
    "href": "posts/pytorch-lm-with-nntransformer-and-torchtext/pytorch-lm-with-nntransformer-and-torchtext.html#load-and-batch-data",
    "title": "Language Modeling with nn.Transformer and Torchtext",
    "section": "Load and batch data",
    "text": "Load and batch data\n이 튜토리얼은 Wikitext-2 dataset를 생성하기 위해 torchtext를 사용한다. torchtext datasets를 사용하기 위해서는 다음과 같은 명령어로 torchdata를 설치한다\n\n%%bash\npip install torchdata\n\nvocab 객체는 train dataset을 기반으로 만들고 토큰을 수치화해서 tensor로 만들기위해 사용된다. Wikitext-2는 희귀한 토큰을 <unk>로 표현한다.\n1-D vector로 이루어진 순서 데이터가 주어지면, batchify()는 데이터를 batch_size column 수만큼 배열한다. 만약 데이터가 batch_size column 수만큼 나누어 떨이지지 않는다면, 데이터를 자른다. 예를들어, 길이가 26인 알파벳 data가 있고 batch_size=4라면, 알파벳을 길이가 6인 sequences 4개를 만든다.\n\\[\n\\begin{bmatrix}\nA & B & C & \\cdots & X & Y & Z\n\\end{bmatrix}\n\\Rightarrow\n\\begin{bmatrix}\n\\begin{bmatrix}\nA \\\\ B \\\\ C \\\\ D \\\\ E \\\\ F\n\\end{bmatrix}\n\\begin{bmatrix}\nG \\\\ H \\\\ I \\\\ J \\\\ K \\\\ L\n\\end{bmatrix}\n\\begin{bmatrix}\nM \\\\ N \\\\ O \\\\ P \\\\ Q \\\\ R\n\\end{bmatrix}\n\\begin{bmatrix}\nS \\\\ T \\\\ U \\\\ V \\\\ W \\\\ X\n\\end{bmatrix}\n\\end{bmatrix}\n\\]\nBatching은 병렬처리를 가능하게 하지만, 각 column을 독립척으로 처리한다. 그래서 G와 F의 의존성은 위의 예에서 학습할 수 없다.\n\nfrom torchtext.datasets import WikiText2\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import build_vocab_from_iterator\n\ntrain_iter = WikiText2(split='train')\ntokenizer = get_tokenizer('basic_english')\nvocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\nvocab.set_default_index(vocab['<unk>'])\n\ndef data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n\n# train_iter was \"consumed\" by the process of building the vocab,\n# so we have to create it again\ntrain_iter, val_iter, test_iter = WikiText2()\ntrain_data = data_process(train_iter)\nval_data = data_process(val_iter)\ntest_data = data_process(test_iter)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ndef batchify(data: Tensor, bsz: int) -> Tensor:\n    \"\"\"Divides the data into bsz separate sequences, removing extra elements\n    that wouldn't cleanly fit.\n\n    Args:\n        data: Tensor, shape [N]\n        bsz: int, batch size\n\n    Returns:\n        Tensor of shape [N // bsz, bsz]\n    \"\"\"\n    seq_len = data.size(0) // bsz\n    data = data[:seq_len * bsz]\n    data = data.view(bsz, seq_len).t().contiguous()\n    return data.to(device)\n\nbatch_size = 20\neval_batch_size = 10\ntrain_data = batchify(train_data, batch_size)  # shape [seq_len, batch_size]\nval_data = batchify(val_data, eval_batch_size)\ntest_data = batchify(test_data, eval_batch_size)\n\n\nFunctions to generate input and target sequence\nget_batch()는 transformer model을 위한 input-target sequences를 생성한다. get_batch()는 소스 데이터를 bptt 길이의 chunks로 세분화한다. Language modeling 작업을 위해, model은 Target이라는 미래 단어들이 필요하다. 예를들어, bptt가 2라면, i=0일 때의 2개의 미래 단어를 얻을 수 있다.\n\n\n\nmatrix\n\n\n(batch마다 2개의 단어를 묶어야 하므로 input (A,B), target (B,C), input (G, H), target (H, I), … 로 row가 아니라 column으로 같이 묶여야 한다. 위의 행렬에 오류 있음.)\nchunks는 데이터의 0번째 차원에 있다는 것을 유의해야한다. batch는 1번째 차원에 있다.\n\nbptt = 35\ndef get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n    \"\"\"\n    Args:\n        source: Tensor, shape [full_seq_len, batch_size]\n        i: int\n\n    Returns:\n        tuple (data, target), where data has shape [seq_len, batch_size] and\n        target has shape [seq_len * batch_size]\n    \"\"\"\n    seq_len = min(bptt, len(source) - 1 - i)\n    data = source[i:i+seq_len]\n    target = source[i+1:i+1+seq_len].reshape(-1)\n    return data, target"
  },
  {
    "objectID": "posts/pytorch-lm-with-nntransformer-and-torchtext/pytorch-lm-with-nntransformer-and-torchtext.html#initiate-an-instance",
    "href": "posts/pytorch-lm-with-nntransformer-and-torchtext/pytorch-lm-with-nntransformer-and-torchtext.html#initiate-an-instance",
    "title": "Language Modeling with nn.Transformer and Torchtext",
    "section": "Initiate an instance",
    "text": "Initiate an instance\nmodel hyperparameters는 아래에 정의되어 있다. vocab size는 vocab object와 같다.\n\nntokens = len(vocab)  # size of vocabulary\nemsize = 200  # embedding dimension\nd_hid = 200  # dimension of the feedforward network model in nn.TransformerEncoder\nnlayers = 2  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\nnhead = 2  # number of heads in nn.MultiheadAttention\ndropout = 0.2  # dropout probability\nmodel = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)"
  },
  {
    "objectID": "posts/pytorch-lm-with-nntransformer-and-torchtext/pytorch-lm-with-nntransformer-and-torchtext.html#run-the-model",
    "href": "posts/pytorch-lm-with-nntransformer-and-torchtext/pytorch-lm-with-nntransformer-and-torchtext.html#run-the-model",
    "title": "Language Modeling with nn.Transformer and Torchtext",
    "section": "Run the model",
    "text": "Run the model\n학습은 SGD (stochastic gradient descent) optimizer를 사용하고 loss function으로 CrossEntropyLoss를 사용한다. learning rate는 5.0으로 초기화하고 StepLR을 사용해 schedule을 한다. 학습하는 동안 nn.utils.clip_grad_norm_을 사용해 gradient exploding을 방지한다.\n\nimport copy\nimport time\n\ncriterion = nn.CrossEntropyLoss()\nlr = 5.0  # learning rate\noptimizer = torch.optim.SGD(model.parameters(), lr=lr)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n\ndef train(model: nn.Module) -> None:\n    model.train()  # turn on train mode\n    total_loss = 0.\n    log_interval = 200\n    start_time = time.time()\n    src_mask = generate_square_subsequent_mask(bptt).to(device)\n\n    num_batches = len(train_data) // bptt\n    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n        data, targets = get_batch(train_data, i)\n        seq_len = data.size(0)\n        if seq_len != bptt:  # only on last batch\n            src_mask = src_mask[:seq_len, :seq_len]\n        output = model(data, src_mask)\n        loss = criterion(output.view(-1, ntokens), targets)\n\n        optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n        optimizer.step()\n\n        total_loss += loss.item()\n        if batch % log_interval == 0 and batch > 0:\n            lr = scheduler.get_last_lr()[0]\n            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n            cur_loss = total_loss / log_interval\n            ppl = math.exp(cur_loss)\n            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n            total_loss = 0\n            start_time = time.time()\n\ndef evaluate(model: nn.Module, eval_data: Tensor) -> float:\n    model.eval()  # turn on evaluation mode\n    total_loss = 0.\n    src_mask = generate_square_subsequent_mask(bptt).to(device)\n    with torch.no_grad():\n        for i in range(0, eval_data.size(0) - 1, bptt):\n            data, targets = get_batch(eval_data, i)\n            seq_len = data.size(0)\n            if seq_len != bptt:\n                src_mask = src_mask[:seq_len, :seq_len]\n            output = model(data, src_mask)\n            output_flat = output.view(-1, ntokens)\n            total_loss += seq_len * criterion(output_flat, targets).item()\n    return total_loss / (len(eval_data) - 1)\n\nepoch를 돌면서 이때까지 본 것 중 validation loss가 가장 좋다면 모델을 저장한다. 또 각 epoch마다 learning rate를 조절한다.\n\nbest_val_loss = float('inf')\nepochs = 3\n\nwith TemporaryDirectory() as tempdir:\n    best_model_params_path = os.path.join(tempdir, \"best_model_params.pt\")\n\n    for epoch in range(1, epochs + 1):\n        epoch_start_time = time.time()\n        train(model)\n        val_loss = evaluate(model, val_data)\n        val_ppl = math.exp(val_loss)\n        elapsed = time.time() - epoch_start_time\n        print('-' * 89)\n        print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n            f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n        print('-' * 89)\n\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            torch.save(model.state_dict(), best_model_params_path)\n\n        scheduler.step()\n    model.load_state_dict(torch.load(best_model_params_path)) # load best model states\n\n| epoch   1 |   200/ 2928 batches | lr 5.00 | ms/batch 26.08 | loss  8.23 | ppl  3737.76\n| epoch   1 |   400/ 2928 batches | lr 5.00 | ms/batch 11.58 | loss  6.92 | ppl  1010.80\n| epoch   1 |   600/ 2928 batches | lr 5.00 | ms/batch 11.46 | loss  6.47 | ppl   645.55\n| epoch   1 |   800/ 2928 batches | lr 5.00 | ms/batch 11.28 | loss  6.32 | ppl   555.72\n| epoch   1 |  1000/ 2928 batches | lr 5.00 | ms/batch 11.52 | loss  6.20 | ppl   493.80\n| epoch   1 |  1200/ 2928 batches | lr 5.00 | ms/batch 11.46 | loss  6.17 | ppl   479.33\n| epoch   1 |  1400/ 2928 batches | lr 5.00 | ms/batch 11.43 | loss  6.12 | ppl   455.35\n| epoch   1 |  1600/ 2928 batches | lr 5.00 | ms/batch 11.58 | loss  6.11 | ppl   451.57\n| epoch   1 |  1800/ 2928 batches | lr 5.00 | ms/batch 11.61 | loss  6.04 | ppl   419.10\n| epoch   1 |  2000/ 2928 batches | lr 5.00 | ms/batch 11.65 | loss  6.02 | ppl   412.79\n| epoch   1 |  2200/ 2928 batches | lr 5.00 | ms/batch 11.67 | loss  5.90 | ppl   364.81\n| epoch   1 |  2400/ 2928 batches | lr 5.00 | ms/batch 11.43 | loss  5.98 | ppl   393.81\n| epoch   1 |  2600/ 2928 batches | lr 5.00 | ms/batch 11.53 | loss  5.96 | ppl   386.26\n| epoch   1 |  2800/ 2928 batches | lr 5.00 | ms/batch 11.36 | loss  5.88 | ppl   356.50\n-----------------------------------------------------------------------------------------\n| end of epoch   1 | time: 38.00s | valid loss  5.78 | valid ppl   325.14\n-----------------------------------------------------------------------------------------\n| epoch   2 |   200/ 2928 batches | lr 4.75 | ms/batch 11.61 | loss  5.87 | ppl   355.89\n| epoch   2 |   400/ 2928 batches | lr 4.75 | ms/batch 11.62 | loss  5.86 | ppl   349.88\n| epoch   2 |   600/ 2928 batches | lr 4.75 | ms/batch 11.48 | loss  5.67 | ppl   288.70\n| epoch   2 |   800/ 2928 batches | lr 4.75 | ms/batch 11.40 | loss  5.71 | ppl   301.53\n| epoch   2 |  1000/ 2928 batches | lr 4.75 | ms/batch 11.54 | loss  5.66 | ppl   287.14\n| epoch   2 |  1200/ 2928 batches | lr 4.75 | ms/batch 11.68 | loss  5.69 | ppl   294.82\n| epoch   2 |  1400/ 2928 batches | lr 4.75 | ms/batch 11.46 | loss  5.69 | ppl   297.15\n| epoch   2 |  1600/ 2928 batches | lr 4.75 | ms/batch 11.45 | loss  5.72 | ppl   303.57\n| epoch   2 |  1800/ 2928 batches | lr 4.75 | ms/batch 11.51 | loss  5.65 | ppl   285.06\n| epoch   2 |  2000/ 2928 batches | lr 4.75 | ms/batch 11.31 | loss  5.66 | ppl   288.08\n| epoch   2 |  2200/ 2928 batches | lr 4.75 | ms/batch 11.41 | loss  5.55 | ppl   258.10\n| epoch   2 |  2400/ 2928 batches | lr 4.75 | ms/batch 11.52 | loss  5.65 | ppl   284.08\n| epoch   2 |  2600/ 2928 batches | lr 4.75 | ms/batch 11.59 | loss  5.64 | ppl   282.13\n| epoch   2 |  2800/ 2928 batches | lr 4.75 | ms/batch 11.51 | loss  5.57 | ppl   262.94\n-----------------------------------------------------------------------------------------\n| end of epoch   2 | time: 35.02s | valid loss  5.67 | valid ppl   290.63\n-----------------------------------------------------------------------------------------\n| epoch   3 |   200/ 2928 batches | lr 4.51 | ms/batch 11.32 | loss  5.60 | ppl   271.62\n| epoch   3 |   400/ 2928 batches | lr 4.51 | ms/batch 11.36 | loss  5.63 | ppl   277.62\n| epoch   3 |   600/ 2928 batches | lr 4.51 | ms/batch 11.55 | loss  5.43 | ppl   227.22\n| epoch   3 |   800/ 2928 batches | lr 4.51 | ms/batch 11.58 | loss  5.48 | ppl   240.90\n| epoch   3 |  1000/ 2928 batches | lr 4.51 | ms/batch 11.22 | loss  5.44 | ppl   229.37\n| epoch   3 |  1200/ 2928 batches | lr 4.51 | ms/batch 11.36 | loss  5.48 | ppl   239.41\n| epoch   3 |  1400/ 2928 batches | lr 4.51 | ms/batch 11.41 | loss  5.49 | ppl   241.50\n| epoch   3 |  1600/ 2928 batches | lr 4.51 | ms/batch 11.32 | loss  5.52 | ppl   248.71\n| epoch   3 |  1800/ 2928 batches | lr 4.51 | ms/batch 11.37 | loss  5.46 | ppl   235.82\n| epoch   3 |  2000/ 2928 batches | lr 4.51 | ms/batch 11.32 | loss  5.48 | ppl   240.72\n| epoch   3 |  2200/ 2928 batches | lr 4.51 | ms/batch 11.28 | loss  5.35 | ppl   211.17\n| epoch   3 |  2400/ 2928 batches | lr 4.51 | ms/batch 11.23 | loss  5.46 | ppl   234.29\n| epoch   3 |  2600/ 2928 batches | lr 4.51 | ms/batch 11.37 | loss  5.46 | ppl   235.63\n| epoch   3 |  2800/ 2928 batches | lr 4.51 | ms/batch 11.36 | loss  5.40 | ppl   221.16\n-----------------------------------------------------------------------------------------\n| end of epoch   3 | time: 34.61s | valid loss  5.62 | valid ppl   276.85\n-----------------------------------------------------------------------------------------"
  },
  {
    "objectID": "posts/pytorch-lm-with-nntransformer-and-torchtext/pytorch-lm-with-nntransformer-and-torchtext.html#evaluate-the-best-model-on-the-test-dataset",
    "href": "posts/pytorch-lm-with-nntransformer-and-torchtext/pytorch-lm-with-nntransformer-and-torchtext.html#evaluate-the-best-model-on-the-test-dataset",
    "title": "Language Modeling with nn.Transformer and Torchtext",
    "section": "Evaluate the best model on the test dataset",
    "text": "Evaluate the best model on the test dataset\n\ntest_loss = evaluate(model, test_data)\ntest_ppl = math.exp(test_loss)\nprint('=' * 89)\nprint(f'| End of training | test loss {test_loss:5.2f} | '\n      f'test ppl {test_ppl:8.2f}')\nprint('=' * 89)\n\n=========================================================================================\n| End of training | test loss  5.53 | test ppl   252.38\n========================================================================================="
  },
  {
    "objectID": "posts/pytorch-datasets/pytorch-datasets.html",
    "href": "posts/pytorch-datasets/pytorch-datasets.html",
    "title": "Pytorch Datasets",
    "section": "",
    "text": "출처 : pytorch-datasets-and-dataloaders\n데이터를 처리하는 코드는 쉽게 지저분해지고 유지보수하기가 어렵다; PyTorch는 읽기 쉬우면서 모듈성을 위해 dataset 코드를 모델 학습 코드와 분리하기를 원했다. Pytorch는 2가지 데이터 기본요소를 제공한다: torch.utils.data.DataLoader and torch.utils.data.Dataset. 이 라이브러리는 가지고 있는 자체 데이터를 쓸 수 있을 뿐만 아니라 사전 로드된 dataset을 사용할 수 있다. Dataset은 샘플과 그에 대응되는 레이블을 저장하고, DataLoader는 Dataset을 iterable로 감싸 샘플에 쉽게 접근하도록 한다.\nPyTorch 도메인 라이브러리들은 수많은 사전로드된 dataset을 제공하며(FashionMNIST와 같은), 이 dataset은 torch.utils.data.Dataset을 상속하고 특정 데이터에 대한 특별한 함수를 구현한다. 이 dataset은 모델을 프로토타이핑 및 벤치마크에 쓸 수 있다. 여기 참고 : Image Datasets, Text Datasets, Audio Datasets"
  },
  {
    "objectID": "posts/pytorch-datasets/pytorch-datasets.html#loading-a-dataset",
    "href": "posts/pytorch-datasets/pytorch-datasets.html#loading-a-dataset",
    "title": "Pytorch Datasets",
    "section": "Loading a Dataset",
    "text": "Loading a Dataset\n예시로 TorchVision의 Fashion-MNIST dataset를 사용하자. Fasion-MNIST는 6만 개의 training 샘플과 만 개의 test 샘플을 포함하는 Zalando(독일의 패션 소매 기업) 기사 이미지 dataset이다. 각 샘플은 28x28 크기의 grayscale 이미지로 구성되어 있고 10개의 클래스 중 하나의 레이블을 가진다.\n다음과 같은 parameters를 가지고 FashionMNIST Dataset을 로드할 수 있다.\n\nroot 는 train/test 데이터가 저장될 경로\ntrain 은 training dataset인 지 test dataset 인지 결정\ndownload=True 는 root 경로에 없을 경우 인터넷으로 다운 받을 지 결정\ntransform과 target_transform은 각각 feature와 label 변환을 지정\n\n\nimport torch\nfrom torch.utils.data import Dataset\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\nimport matplotlib.pyplot as plt\n\n\ntraining_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=ToTensor()\n)\n\ntest_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=False,\n    download=True,\n    transform=ToTensor()\n)"
  },
  {
    "objectID": "posts/pytorch-datasets/pytorch-datasets.html#iterating-and-visualizing-the-dataset",
    "href": "posts/pytorch-datasets/pytorch-datasets.html#iterating-and-visualizing-the-dataset",
    "title": "Pytorch Datasets",
    "section": "Iterating and Visualizing the Dataset",
    "text": "Iterating and Visualizing the Dataset\nlist처럼 Datasets에 index로 접근할 수 있다. matplotlib을 사용해 몇몇 샘플을 살펴보면 다음과 같다.\n\nlabels_map = {\n    0: \"T-Shirt\",\n    1: \"Trouser\",\n    2: \"Pullover\",\n    3: \"Dress\",\n    4: \"Coat\",\n    5: \"Sandal\",\n    6: \"Shirt\",\n    7: \"Sneaker\",\n    8: \"Bag\",\n    9: \"Ankle Boot\",\n}\nfigure = plt.figure(figsize=(8, 8))\ncols, rows = 3, 3\nfor i in range(1, cols * rows + 1):\n    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n    img, label = training_data[sample_idx]\n    figure.add_subplot(rows, cols, i)\n    plt.title(labels_map[label])\n    plt.axis(\"off\")\n    plt.imshow(img.squeeze(), cmap=\"gray\")\nplt.show()"
  },
  {
    "objectID": "posts/pytorch-datasets/pytorch-datasets.html#creating-a-custom-dataset-for-your-files",
    "href": "posts/pytorch-datasets/pytorch-datasets.html#creating-a-custom-dataset-for-your-files",
    "title": "Pytorch Datasets",
    "section": "Creating a Custom Dataset for your files",
    "text": "Creating a Custom Dataset for your files\nDataset class를 커스텀 하려면 3개의 함수를 반드시 구현해야 한다: __init__, __len__, 그리고 __getitem__. FashionMNIST 이미지는 img_dir에 저장되어 있고, 레이블은 annotations_file에 저장되어 있자고 하자.\n\n__init__ 함수는 Dataset 객체를 초기화할 때 한번 실행된다. 이미지와 레이블 파일이 있는 directory와 transform을 초기화한다.\n__len__ 함수는 dataset의 샘플 수를 리턴한다.\n__getitem__ 함수는 idx를 가지고 dataset의 하나의 샘플을 로드하고 리턴한다. index를 가지고 이미지의 위치를 찾은 다음, read_image를 사용해 tensor로 변환한다. 그 다음에는 대응되는 레이블을 self.img_labels에서 찾은 후 적용할 transfrom 함수고 있다면 적용한다. 마지막에 이미지 tensor와 레이블 tuple을 리턴한다.\n\n\nimport os\nimport pandas as pd\nfrom torchvision.io import read_image\n\nclass CustomImageDataset(Dataset):\n    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n        self.img_labels = pd.read_csv(annotations_file)\n        self.img_dir = img_dir\n        self.transform = transform\n        self.target_transform = target_transform\n\n    def __len__(self):\n        return len(self.img_labels)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n        image = read_image(img_path)\n        label = self.img_labels.iloc[idx, 1]\n        if self.transform:\n            image = self.transform(image)\n        if self.target_transform:\n            label = self.target_transform(label)\n        return image, label"
  },
  {
    "objectID": "posts/pytorch-datasets/pytorch-datasets.html#preparing-your-data-for-training-with-dataloaders",
    "href": "posts/pytorch-datasets/pytorch-datasets.html#preparing-your-data-for-training-with-dataloaders",
    "title": "Pytorch Datasets",
    "section": "Preparing your data for training with DataLoaders",
    "text": "Preparing your data for training with DataLoaders\nDataset은 dataset의 features와 label을 한번에 하나씩 찾는다. 모델 학습중에는 샘플들을 minibatches로 전달하거나, 모델의 오버피팅을 줄이기 위해 epoch마다 데이터를 reshuffle하거나, 더 빠른 데이터 검색을 위해 Python의 multiprocessing을 사용하고 싶기도 한다.\nDataLoader는 이 기능들을 쉽게 구현한 iterable이다.\n\nfrom torch.utils.data import DataLoader\n\ntrain_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\ntest_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
  },
  {
    "objectID": "posts/pytorch-datasets/pytorch-datasets.html#iterate-through-the-dataloader",
    "href": "posts/pytorch-datasets/pytorch-datasets.html#iterate-through-the-dataloader",
    "title": "Pytorch Datasets",
    "section": "Iterate through the DataLoader",
    "text": "Iterate through the DataLoader\ndataset을 DataLoader에 전달하면 필요한 만큼 반복할 수 있다. 각 반복(iteration)마다 train_features와 train_labels batch를 리턴한다. (batch_size=64 features와 labels 각각). shuffle=True로 지정하면 모든 배치가 끝날때 마다 데이터가 셔플된다. (finer-grained control은 Samplers 참고)\n\n# Display image and label.\ntrain_features, train_labels = next(iter(train_dataloader))\nprint(f\"Feature batch shape: {train_features.size()}\")\nprint(f\"Labels batch shape: {train_labels.size()}\")\nimg = train_features[0].squeeze()\nlabel = train_labels[0]\nplt.imshow(img, cmap=\"gray\")\nplt.show()\nprint(f\"Label: {label}\")\n\nFeature batch shape: torch.Size([64, 1, 28, 28])\nLabels batch shape: torch.Size([64])\n\n\n\n\n\nLabel: 4"
  },
  {
    "objectID": "posts/pytorch-quickstart/quickstart.html",
    "href": "posts/pytorch-quickstart/quickstart.html",
    "title": "Pytorch Quickstart",
    "section": "",
    "text": "출처 : pytorch 튜토리얼\n이 포스트는 머신러닝의 일반적인 작업에 대한 pytorch api를 살펴본다."
  },
  {
    "objectID": "posts/pytorch-quickstart/quickstart.html#working-with-data",
    "href": "posts/pytorch-quickstart/quickstart.html#working-with-data",
    "title": "Pytorch Quickstart",
    "section": "Working with data",
    "text": "Working with data\npytorch에는 데이터를 다루는데 2가지 기본 요소가 있다: torch.utils.data.DataLoader와 torch.utils.data.Dataset. Dataset은 샘플과 그에 대응되는 레이블을 저장하고, DataLoader는 Dataset의 python iterable 객체이다.\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\n\npytorch는 TorchText, TorchVision, TorchAudio와 같은 도메인에 특화한 라이브러리를 제공하고 각각 데이터셋을 포함한다. 이 튜토리얼에서는 TorchVision 데이터셋을 사용한다.\ntorchvision.datasets 모듈은 CIFAR, COCO (모든 리스트)와 같은 실제 vision 데이터의 Datasets 객체를 포함하고 있다. 이 튜토리얼에서는 FashionMNIST dataset을 사용한다. 모든 TorchVision Dataset은 두 가지 arguments를 포함한다: 샘플과 레이블 각각 수정하기 위한 transform과 target_transform.\n\n# Download training data from open datasets.\ntraining_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=ToTensor(),\n)\n\n# Download test data from open datasets.\ntest_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=False,\n    download=True,\n    transform=ToTensor(),\n)\n\nDataLoader의 argument로 Dataset을 전달한다. DataLoader는 dataset의 iterable을 감싸고, 자동 배치, 샘플링, 셔플링 및 멀티프로세싱을 지원한다. 배치 사이즈를 64로 지정하면 dataloader는 64개의 피쳐와 레이블 배치를 리턴한다.\n\nbatch_size = 64\n\n# Create data loaders.\ntrain_dataloader = DataLoader(training_data, batch_size=batch_size)\ntest_dataloader = DataLoader(test_data, batch_size=batch_size)\n\nfor X, y in test_dataloader:\n    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n    print(f\"Shape of y: {y.shape} {y.dtype}\")\n    break\n\nShape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\nShape of y: torch.Size([64]) torch.int64"
  },
  {
    "objectID": "posts/pytorch-quickstart/quickstart.html#creating-models",
    "href": "posts/pytorch-quickstart/quickstart.html#creating-models",
    "title": "Pytorch Quickstart",
    "section": "Creating Models",
    "text": "Creating Models\npytorch에서 뉴럴 네트워크를 정의하려면 nn.Module을 상속하는 class를 만들어야한다. __init__ function에서 네트워크의 레이어를 정의하고 forward function에서 데이터가 어떻게 통과할 지 정의한다. 연산을 가속화하기 위해 GPU를 사용할 수 있다.\n자세한 내용은 building neural networks in PyTorch 참고.\n\n# Get cpu or gpu device for training.\ndevice = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\nprint(f\"Using {device} device\")\n\n# Define model\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10)\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\nmodel = NeuralNetwork().to(device)\nprint(model)\n\nUsing cuda device\nNeuralNetwork(\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (linear_relu_stack): Sequential(\n    (0): Linear(in_features=784, out_features=512, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=512, out_features=512, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=512, out_features=10, bias=True)\n  )\n)"
  },
  {
    "objectID": "posts/pytorch-quickstart/quickstart.html#optimizing-the-model-parameters",
    "href": "posts/pytorch-quickstart/quickstart.html#optimizing-the-model-parameters",
    "title": "Pytorch Quickstart",
    "section": "Optimizing the Model Parameters",
    "text": "Optimizing the Model Parameters\n모델을 학습하기 위해 loss function과 optimizer가 필요하다.\n\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n\n한번의 학습 루프에서 모델은 학습 dataset에 대해 predictions를 만들고, 모델의 parameters를 조정하기 위해 prediction error를 backpropagate를 한다.\n\ndef train(dataloader, model, loss_fn, optimizer):\n    size = len(dataloader.dataset)\n    model.train()\n    for batch, (X, y) in enumerate(dataloader):\n        X, y = X.to(device), y.to(device)\n\n        # Compute prediction error\n        pred = model(X)\n        loss = loss_fn(pred, y)\n\n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), (batch + 1) * len(X)\n            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n\n학습이 잘 되고 있는지 test dataset에 대해 모델의 성능 측정도 해야한다.\n\ndef test(dataloader, model, loss_fn):\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    model.eval()\n    test_loss, correct = 0, 0\n    with torch.no_grad():\n        for X, y in dataloader:\n            X, y = X.to(device), y.to(device)\n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n    test_loss /= num_batches\n    correct /= size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n\n학습은 여러번 실행된다. 각 epoch마다 모델은 더 나은 predictions를 위해 parameters를 조정한다. epoch 마다 accuracy의 증가와 loss의 감소를 볼 수 있다.\n\nepochs = 5\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train(train_dataloader, model, loss_fn, optimizer)\n    test(test_dataloader, model, loss_fn)\nprint(\"Done!\")"
  },
  {
    "objectID": "posts/pytorch-quickstart/quickstart.html#saving-models",
    "href": "posts/pytorch-quickstart/quickstart.html#saving-models",
    "title": "Pytorch Quickstart",
    "section": "Saving Models",
    "text": "Saving Models\n모델을 저장하는 방법 중 하나는 내부 상태 dictionary를 직렬화하는 것이다. 이 dictionary에는 모델 paramters를 포함하고 있다.\n\ntorch.save(model.state_dict(), \"model.pth\")\nprint(\"Saved PyTorch Model State to model.pth\")\n\nSaved PyTorch Model State to model.pth"
  },
  {
    "objectID": "posts/pytorch-quickstart/quickstart.html#loading-models",
    "href": "posts/pytorch-quickstart/quickstart.html#loading-models",
    "title": "Pytorch Quickstart",
    "section": "Loading Models",
    "text": "Loading Models\n모델을 load하려면 모델의 구조를 다시 만들고 상태 dictionary를 load한다.\n더 자세한 내용은 Saving & Loading your model 참고.\n\nmodel = NeuralNetwork()\nmodel.load_state_dict(torch.load(\"model.pth\"))\n\n<All keys matched successfully>\n\n\n\nclasses = [\n    \"T-shirt/top\",\n    \"Trouser\",\n    \"Pullover\",\n    \"Dress\",\n    \"Coat\",\n    \"Sandal\",\n    \"Shirt\",\n    \"Sneaker\",\n    \"Bag\",\n    \"Ankle boot\",\n]\n\nmodel.eval()\nx, y = test_data[0][0], test_data[0][1]\nwith torch.no_grad():\n    pred = model(x)\n    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')\n\nPredicted: \"Ankle boot\", Actual: \"Ankle boot\""
  },
  {
    "objectID": "posts/pytorch-tensors/pytorch-tensors.html",
    "href": "posts/pytorch-tensors/pytorch-tensors.html",
    "title": "Pytorch Tensors",
    "section": "",
    "text": "출처 : Pytorch Tensors\nTensor는 배열과 행렬과 비슷한 특별한 자료구조이다. PyTorch에서 모델의 input, output과 parameter를 인코딩하는데 tensor를 사용한다.\nTensor는 Numpy의 ndarrays와 비슷하지만, tensor는 GPU 가속이 가능하다. 또 tensor는 자동 미분(automatic differentiation)에 최적화되어있다."
  },
  {
    "objectID": "posts/pytorch-tensors/pytorch-tensors.html#initializing-a-tensor",
    "href": "posts/pytorch-tensors/pytorch-tensors.html#initializing-a-tensor",
    "title": "Pytorch Tensors",
    "section": "Initializing a Tensor",
    "text": "Initializing a Tensor\nTensor는 다양한 방법으로 초기화할 수 있다. ### Directly from data Data로 부터 바로 tensor를 만들 수 있다. 여기서 데이터 타입은 자동으로 추론된다.\n\ndata = [[1, 2],[3, 4]]\nx_data = torch.tensor(data)\n\n\nFrom a NumPy array\nNumPy array로 부터 tensor를 만들 수 있다. (반대 방향도 가능)\n\nnp_array = np.array(data)\nx_np = torch.from_numpy(np_array)\n\n\n\nFrom another tensor:\n다른 tensor의 속성(shape, datatype)으로 부터 새 tensor를 만들 수 있다.\n\nx_ones = torch.ones_like(x_data) # retains the properties of x_data\nprint(f\"Ones Tensor: \\n {x_ones} \\n\")\n\nx_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\nprint(f\"Random Tensor: \\n {x_rand} \\n\")\n\nOnes Tensor: \n tensor([[1, 1],\n        [1, 1]]) \n\nRandom Tensor: \n tensor([[0.3985, 0.1358],\n        [0.7151, 0.1586]]) \n\n\n\n\n\nWith random or constant values:\nshape를 지정해서 tensor를 만들 수 있다.\n\nshape = (2,3,)\nrand_tensor = torch.rand(shape)\nones_tensor = torch.ones(shape)\nzeros_tensor = torch.zeros(shape)\n\nprint(f\"Random Tensor: \\n {rand_tensor} \\n\")\nprint(f\"Ones Tensor: \\n {ones_tensor} \\n\")\nprint(f\"Zeros Tensor: \\n {zeros_tensor}\")\n\nRandom Tensor: \n tensor([[0.9729, 0.5650, 0.1026],\n        [0.8679, 0.1286, 0.8256]]) \n\nOnes Tensor: \n tensor([[1., 1., 1.],\n        [1., 1., 1.]]) \n\nZeros Tensor: \n tensor([[0., 0., 0.],\n        [0., 0., 0.]])"
  },
  {
    "objectID": "posts/pytorch-tensors/pytorch-tensors.html#attributes-of-a-tensor",
    "href": "posts/pytorch-tensors/pytorch-tensors.html#attributes-of-a-tensor",
    "title": "Pytorch Tensors",
    "section": "Attributes of a Tensor",
    "text": "Attributes of a Tensor\nTensor 속성으로 shape, datatype, 그리고 어느 device에 저장되어있는 지가 있다.\n\ntensor = torch.rand(3,4)\n\nprint(f\"Shape of tensor: {tensor.shape}\")\nprint(f\"Datatype of tensor: {tensor.dtype}\")\nprint(f\"Device tensor is stored on: {tensor.device}\")\n\nShape of tensor: torch.Size([3, 4])\nDatatype of tensor: torch.float32\nDevice tensor is stored on: cpu"
  },
  {
    "objectID": "posts/pytorch-tensors/pytorch-tensors.html#operations-on-tensors",
    "href": "posts/pytorch-tensors/pytorch-tensors.html#operations-on-tensors",
    "title": "Pytorch Tensors",
    "section": "Operations on Tensors",
    "text": "Operations on Tensors\nTensor의 연산으로 산술(arithmetic), 선형대수, 행렬 조작(transposing, indexing, slicing), 샘플링 등 100개가 넘는 연산이 있다.\n더 자세한 건 여기 참고\n각 연산은 GPU에서 실행할 수 있다. 기본값으로 tensor는 CPU에서 생성된다. GPU를 사용하려면 명시적으로 .to 메소드를 사용해 tensor를 GPU로 이동시켜야 한다(GPU를 사용할 수 있는 지 체크한 이후에). 거대한 tensor를 devices 간에 복사하는 건 큰 비용이 드는 것을 명심해야한다.\n\n# We move our tensor to the GPU if available\nif torch.cuda.is_available():\n    tensor = tensor.to(\"cuda\")\n\n몇몇 연산을 살펴보는데, NumPy API에 친숙하다면 Tensor API는 사용하기 쉬울 것이다. ### Standard numpy-like indexing and slicing:\n\ntensor = torch.ones(4, 4)\nprint(f\"First row: {tensor[0]}\")\nprint(f\"First column: {tensor[:, 0]}\")\nprint(f\"Last column: {tensor[..., -1]}\")\ntensor[:,1] = 0\nprint(tensor)\n\nFirst row: tensor([1., 1., 1., 1.])\nFirst column: tensor([1., 1., 1., 1.])\nLast column: tensor([1., 1., 1., 1.])\ntensor([[1., 0., 1., 1.],\n        [1., 0., 1., 1.],\n        [1., 0., 1., 1.],\n        [1., 0., 1., 1.]])\n\n\n\nJoining tensors\ntorch.cat을 사용하면 tensor들의 시퀀스를 합칠(concatenate) 수 있다. torch.stack은 torch.cat과 다른 합치는 연산이다.\n\nt1 = torch.cat([tensor, tensor, tensor], dim=1)\nprint(t1)\n\ntensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n\n\n\n\nArithmetic operations\n\n# This computes the matrix multiplication between two tensors.\n# y1, y2, y3 will have the same value\n# ``tensor.T`` returns the transpose of a tensor\ny1 = tensor @ tensor.T\ny2 = tensor.matmul(tensor.T)\n\ny3 = torch.rand_like(y1)\ntorch.matmul(tensor, tensor.T, out=y3)\n\n\n# This computes the element-wise product. z1, z2, z3 will have the same value\nz1 = tensor * tensor\nz2 = tensor.mul(tensor)\n\nz3 = torch.rand_like(tensor)\ntorch.mul(tensor, tensor, out=z3)\n\ntensor([[1., 0., 1., 1.],\n        [1., 0., 1., 1.],\n        [1., 0., 1., 1.],\n        [1., 0., 1., 1.]])\n\n\n\n\nSingle-element tensors\n만약에 한 개의 원소를 가진 tensor가 있다면, 예를 들어 모든 값들을 집계해서 하나의 값을 가진 tensor를 만든다면, item()을 사용해서 Python 값(variable)로 변환할 수 있다.\n\nagg = tensor.sum()\nagg_item = agg.item()\nprint(agg_item, type(agg_item))\n\n12.0 <class 'float'>\n\n\n\n\nIn-place operations\n연산의 결과로 피연산자(operand)에 저장되는 것을 in-place라 한다. inplace 연산은 _ suffix로 나타낸다. 예: x.copey_(y), x.t_() 는 x를 바꾼다. In-place 연산은 메모리를 아끼지만, 미분을 계산하는데 문제가 생길 수 있다. 그래서 사용이 권장되지 않는다.\n\nprint(f\"{tensor} \\n\")\ntensor.add_(5)\nprint(tensor)\n\ntensor([[1., 0., 1., 1.],\n        [1., 0., 1., 1.],\n        [1., 0., 1., 1.],\n        [1., 0., 1., 1.]]) \n\ntensor([[6., 5., 6., 6.],\n        [6., 5., 6., 6.],\n        [6., 5., 6., 6.],\n        [6., 5., 6., 6.]])"
  },
  {
    "objectID": "posts/pytorch-tensors/pytorch-tensors.html#bridge-with-numpy",
    "href": "posts/pytorch-tensors/pytorch-tensors.html#bridge-with-numpy",
    "title": "Pytorch Tensors",
    "section": "Bridge with Numpy",
    "text": "Bridge with Numpy\nCPU에 있는 tensor와 Numpy array는 메모리 위치를 공유하고 있어서, 하나를 바꾼다면 다른 하나도 바뀐다.\n\nt = torch.ones(5)\nprint(f\"t: {t}\")\nn = t.numpy()\nprint(f\"n: {n}\")\n\nt: tensor([1., 1., 1., 1., 1.])\nn: [1. 1. 1. 1. 1.]\n\n\n\nt.add_(1)\nprint(f\"t: {t}\")\nprint(f\"n: {n}\")\n\nt: tensor([2., 2., 2., 2., 2.])\nn: [2. 2. 2. 2. 2.]\n\n\n\nNumpy array to Tensor\n\nn = np.ones(5)\nt = torch.from_numpy(n)\nprint(f\"t: {t}\")\nprint(f\"n: {n}\")\n\nt: tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\nn: [1. 1. 1. 1. 1.]\n\n\n\nnp.add(n, 1, out=n)\nprint(f\"t: {t}\")\nprint(f\"n: {n}\")\n\nt: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\nn: [2. 2. 2. 2. 2.]"
  }
]